seed: 0
debug: false
save_checkpoint: true
save_rand_checkpoint: false
early_stopping: true
model:
  optimizer:
    _target_: transformers.AdamW
    lr: 1.0e-05
    betas:
    - 0.9
    - 0.98
    eps: 1.0e-08
    weight_decay: 0.0
    correct_bias: false
  scheduler:
    lr_scheduler: linear_with_warmup
    warmup_updates: 0.1
  _target_: multi_emotion_recognition.model.lm.MultiEmoModel
  arch: bert-base-uncased
  dataset: ${data.dataset}
  neg_weight: 1
  num_freeze_layers: 0
  freeze_epochs: -1
  save_outputs: ${training.save_outputs}
  exp_id: null
  measure_attrs_runtime: false
  use_hashtag: true
  use_senti_tree: true
  use_emo_cor: true
  hashtag_emb_dim: 80
  phrase_emb_dim: 80
data:
  _target_: multi_emotion_recognition.data.data.DataModule
  dataset: se_english
  num_workers: ${setup.num_workers}
  data_path: ${paths.data_dir}/${.dataset}/${model.arch}/
  train_batch_size: ${setup.train_batch_size}
  eval_batch_size: ${setup.eval_batch_size}
  eff_train_batch_size: ${setup.eff_train_batch_size}
  mode: max
  num_train: null
  num_dev: null
  num_test: null
  num_train_seed: 0
  num_dev_seed: 0
  num_test_seed: 0
  use_hashtag: ${model.use_hashtag}
  use_senti_tree: ${model.use_senti_tree}
  phrase_num: 20
logger:
  _target_: multi_emotion_recognition.utils.logging.get_csv_logger
  logger: csv
  save_dir: ${paths.save_dir}
  name: ${data.dataset}
  offline: true
setup:
  train_batch_size: 2
  eval_batch_size: 2
  accumulate_grad_batches: 1
  eff_train_batch_size: 2
  num_workers: 1
  precision: 16
paths:
  root_dir: ${oc.env:PROJECT_ROOT}
  data_dir: ${paths.root_dir}/data/
  log_dir: ${paths.root_dir}/logs/
  save_dir: ${paths.root_dir}/save/
  work_dir: ${hydra:runtime.cwd}
training:
  evaluate_ckpt: false
  finetune_ckpt: false
  save_outputs: false
  ckpt_path: null
  eval_splits: all
  train_shuffle: true
  patience: 3
trainer:
  _target_: pytorch_lightning.Trainer
  checkpoint_callback: true
  default_root_dir: null
  gradient_clip_val: 0
  process_position: 0
  num_nodes: 1
  num_processes: 1
  gpus: -1
  auto_select_gpus: true
  tpu_cores: null
  log_gpu_memory: null
  progress_bar_refresh_rate: 1
  overfit_batches: 0.0
  track_grad_norm: -1
  check_val_every_n_epoch: 1
  fast_dev_run: false
  accumulate_grad_batches: ${setup.accumulate_grad_batches}
  max_epochs: 10
  min_epochs: 1
  max_steps: null
  min_steps: null
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  limit_test_batches: 1.0
  val_check_interval: 1.0
  flush_logs_every_n_steps: 1000
  log_every_n_steps: 1000
  accelerator: null
  sync_batchnorm: false
  precision: ${setup.precision}
  weights_summary: top
  weights_save_path: null
  num_sanity_val_steps: 0
  truncated_bptt_steps: null
  resume_from_checkpoint: null
  profiler: null
  benchmark: true
  deterministic: true
  reload_dataloaders_every_epoch: false
  auto_lr_find: false
  replace_sampler_ddp: true
  terminate_on_nan: false
  auto_scale_batch_size: false
  prepare_data_per_node: true
  plugins: null
  amp_backend: native
  amp_level: O2
  move_metrics_to_cpu: false
