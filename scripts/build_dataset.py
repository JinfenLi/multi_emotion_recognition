"""
    :author: Jinfen Li
    :url: https://github.com/JinfenLi
"""
import argparse, json, math, os, sys, random, logging
import pickle
import re

import datasets
from collections import defaultdict as ddict, Counter

import emotlib
import nltk
import numpy as np
import pandas as pd
import torch
from nltk import TweetTokenizer
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModel
from stanfordnlp.server import CoreNLPClient

from dotenv import load_dotenv
import os
load_dotenv(override=True)
logging.basicConfig(level=logging.DEBUG, format='%(relativeCreated)6d %(threadName)s %(message)s')
logger = logging.getLogger(__name__)

import pyrootutils
pyrootutils.setup_root(__file__, indicator=".project-root", pythonpath=True)
from multi_emotion_recognition.utils.data import dataset_info, data_keys
from multi_emotion_recognition.utils.sentiTree.data_utils import nrc_hashtag_lexicon, spanish_hashtag_lexicon, get_hashtag_inputs
os.environ["CORENLP_HOME"] = os.environ.get("CORENLP_HOME")
print(os.environ["CORENLP_HOME"])
def set_random_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def stratified_sampling(data, num_samples):
    num_instances = len(data)
    assert num_samples < num_instances

    counter_dict = Counter(data)
    unique_vals = list(counter_dict.keys())
    val_counts = list(counter_dict.values())
    num_unique_vals = len(unique_vals)
    assert num_unique_vals > 1

    num_stratified_samples = [int(c*num_samples/num_instances) for c in val_counts]
    assert sum(num_stratified_samples) <= num_samples
    if sum(num_stratified_samples) < num_samples:
        delta = num_samples - sum(num_stratified_samples)
        delta_samples = np.random.choice(range(num_unique_vals), replace=True, size=delta)
        for val in delta_samples:
            num_stratified_samples[unique_vals.index(val)] += 1
    assert sum(num_stratified_samples) == num_samples

    sampled_indices = []
    for i, val in enumerate(unique_vals):
        candidates = np.where(data == val)[0]
        sampled_indices += list(np.random.choice(candidates, replace=False, size=num_stratified_samples[i]))
    random.shuffle(sampled_indices)

    return sampled_indices
def sample_dataset(data_path, dataset_dict, split, num_samples, seed):
    sampled_split_filename = f'{split}_split_{num_samples}_{seed}.pkl'
    if os.path.exists(os.path.join(data_path, sampled_split_filename)):
        with open(os.path.join(data_path, sampled_split_filename), 'rb') as f:
            sampled_split = pickle.load(f)
    else:
        sampled_split = stratified_sampling(dataset_dict['label'], num_samples)
        with open(os.path.join(data_path, sampled_split_filename), 'wb') as f:
            pickle.dump(sampled_split, f)

    for key in data_keys:
        dataset_dict[key] = sampled_split if key == 'item_idx' else [dataset_dict[key][i] for i in sampled_split]

    return dataset_dict


def remap_token_id(reference_offset, parsed_offset, ref_offset_tid = 1):
    """

    Args:
        reference_offset: offset generated by original model
        parsed_offset: offset generated by stanfordnlp
        ref_offset_tid: initial reference_offset token id, the first reference_offset is for [CLS]

    Returns: {new_token_dict: [start_ref_tid, end_ref_tid)}, end_ref_tid is not inclusive

    """
    parsed_offset_tid = 0
    new_token_dict = {}
    start = ref_offset_tid
    while parsed_offset_tid < len(parsed_offset):
        if reference_offset[ref_offset_tid][0] == 0 and reference_offset[ref_offset_tid][1] == 0:
            break
        elif parsed_offset[parsed_offset_tid][1] >= reference_offset[ref_offset_tid][1]:
            # find the first reference_offset that exceed parsed_offset
            while ref_offset_tid < (len(reference_offset) - 1) and parsed_offset[parsed_offset_tid][1] > reference_offset[ref_offset_tid][
                1]:
                ref_offset_tid += 1
            if parsed_offset_tid > 0:
                start = new_token_dict[parsed_offset_tid - 1][1]
            end = ref_offset_tid + 1
            new_token_dict[parsed_offset_tid] = [start, end]
            parsed_offset_tid += 1
        elif parsed_offset[parsed_offset_tid][1] < reference_offset[ref_offset_tid][1]:
            if parsed_offset_tid > 0:
                start = new_token_dict[parsed_offset_tid - 1][1]
            end = ref_offset_tid + 1
            new_token_dict[parsed_offset_tid] = [start, end]
            parsed_offset_tid += 1

    return new_token_dict, ref_offset_tid + 1

def parse_senti_tree(senti_tree_str, tid=0):
    """
    Args:
        senti_tree_str: a string of sentiment tree
        tid: initial token id
    Returns: phrase_dict: {'sentiment': [1, 2], 'tokens': [[1, 2], [1, 2]]}

    """
    senti_nodes = [chars.strip() for chars in senti_tree_str.split('\r\n')]
    phrase_dict = ddict(list)
    # stack: [{label: str, sentiment: int, span: list, left_count: int, right_count: int}]

    stack = []

    for node in senti_nodes:
        labels = [l.split("|")[0] for l in node.split('(') if l]
        sentiments = [int(s) for s in re.findall(r"sentiment=(\d)", node)]

        if len(labels) == 3:
            stack.append({"label": labels[0], "sentiment": sentiments[0], "span": [tid, tid + 1], "left_count": node.count("("), "right_count": node.count(")")})
            tid += 2
        elif len(labels) == 2:
            stack.append(
                {"label": labels[0], "sentiment": sentiments[0], "span": [tid], "left_count": node.count("("),
                 "right_count": node.count(")")})
            tid += 1
        elif len(labels) == 1:
            stack.append({"label": labels[0], "sentiment": sentiments[0], "span": [tid] if node.count(")") > 0 else None, "left_count": node.count("("), "right_count": node.count(")")})
            if node.count(")") > 0:
                tid += 1
        # if right_count > left_count, means rolling back
        while stack and stack[-1]["right_count"] > stack[-1]["left_count"]:
            element = stack.pop()
            stack[-1]["right_count"] = stack[-1]["right_count"] + element["right_count"]
            stack[-1]["left_count"] = stack[-1]["left_count"] + element["left_count"]
            # the last element is the left node
            if stack[-1]["span"] is not None and len(stack[-1]["span"]) == 2:
                continue
            stack[-1]["span"] = [stack[-1]["span"][0] if stack[-1]["span"] is not None else element["span"][0], tid - 1]
            if stack[-1]["label"] in ['@S', 'S', 'ROOT']:
                phrase_dict['sentiment'].append(stack[-1]["sentiment"])
                phrase_dict['tokens'].append(stack[-1]["span"])

    return phrase_dict


def sentiment_tree(texts, num_examples, original_offsets, max_length):
    sents_with_tokens_list = []

    with CoreNLPClient(be_quiet=False, annotators=['tokenize', 'sentiment'], properties={'tokenize.codepoint': 'true'}, timeout=60000,
                       memory='16G', output_format='json') as client:

        for idx in tqdm(range(num_examples), desc=f'Building {args.split} dataset'):
            new_senti_dict = ddict(list)
            text = texts[idx]
            original_offset = original_offsets[idx]
            ann = client.annotate(text.strip())
            cur_start_tid = 0
            result_dict = ddict(list)
            new_token_dict, phrase_dict = {}, {}
            cur_start_ref_tid = 1
            for sent_dict in ann['sentences']:

                sent_dict['offset'] = [(x['codepointOffsetBegin'], x['codepointOffsetEnd']) for x in sent_dict['tokens']]
                # original_token_idx = [x['index'] for x in sent_dict['tokens']]
                remap_token_dict, cur_start_ref_tid = remap_token_id(original_offset, sent_dict['offset'], cur_start_ref_tid)
                new_token_dict.update({k + cur_start_tid: v for k, v in remap_token_dict.items()})
                phrase_dict = parse_senti_tree(sent_dict['sentimentTree'], cur_start_tid)
                phrase_dict['tokens'] = [[new_token_dict[x[0]][0], new_token_dict[x[1]][1]] for x in phrase_dict['tokens']]
                result_dict['spans'].extend(phrase_dict['tokens'])
                result_dict['sentiment'].extend(phrase_dict['sentiment'])
                cur_start_tid = max(new_token_dict.keys()) + 1
            sents_with_tokens_list.append(result_dict)


    return sents_with_tokens_list


def preprocess_dataset(text):
    text = text.replace("\\n", " ")
    text = text.lower()
    text = emotlib.demojify(text)
    return text

def save_datadict(data_path, dataset_dict, split, num_samples, seed):
    for key in tqdm(data_keys, desc=f'Saving {split} dataset'):
        if key in dataset_dict:
            filename = f'{key}.pkl' if num_samples is None else f'{key}_{num_samples}_{seed}.pkl'
            with open(os.path.join(data_path, filename), 'wb') as f:
                pickle.dump(dataset_dict[key], f)

def load_datadict(data_path, split, num_samples, seed):
    dataset_dict = {}
    for key in tqdm(data_keys, desc=f'Loading {split} dataset'):
        if key in dataset_dict:
            filename = f'{key}.pkl' if num_samples is None else f'{key}_{num_samples}_{seed}.pkl'
            with open(os.path.join(data_path, filename), 'rb') as f:
                dataset_dict[key] = pickle.load(f)
    return dataset_dict

def update_dataset_dict(
    idx, dataset_dict, input_ids, hashtag_inputs, max_length, actual_max_length, tokenizer, label, text, token_offsets):
    input_ids = [tokenizer.cls_token_id] + input_ids + [tokenizer.sep_token_id]
    hashtag_inputs = [0] + hashtag_inputs + [0]
    num_tokens = len(input_ids)
    truncated_pos = token_offsets[-1][1]
    if num_tokens > actual_max_length:
        actual_max_length = num_tokens

    num_pad_tokens = max_length - num_tokens
    assert num_pad_tokens >= 0

    input_ids += [tokenizer.pad_token_id] * num_pad_tokens
    attention_mask = [1] * num_tokens + [0] * num_pad_tokens
    hashtag_inputs += [0] * num_pad_tokens
    if token_offsets is not None:
        token_offsets = [(0, 0)] + token_offsets + [(0, 0)]
        token_offsets += [(0, 0)] * num_pad_tokens


    dataset_dict['item_idx'].append(idx)
    dataset_dict['input_ids'].append(input_ids)
    dataset_dict['hashtag_ids'].append(hashtag_inputs)
    dataset_dict['attention_mask'].append(attention_mask)
    dataset_dict['label'].append(label)
    dataset_dict['offsets'].append(token_offsets)
    dataset_dict['truncated_texts'].append(text[: truncated_pos])

    return dataset_dict, actual_max_length

def main():

    set_random_seed(args.seed)

    assert args.split is not None and args.arch is not None
    assert args.num_samples is None or args.num_samples >= 1

    split, num_examples = dataset_info[args.dataset][args.split]
    if args.num_samples is not None:
        assert args.num_samples < num_examples
        num_examples = args.num_samples
    max_length = dataset_info[args.dataset]['max_length'][args.arch]
    num_special_tokens = dataset_info[args.dataset]['num_special_tokens']
    tokenizer = AutoTokenizer.from_pretrained(args.arch, strip_accents=False)
    data_path = os.path.join(args.data_dir, args.dataset, args.arch, args.split)
    classes = dataset_info[args.dataset]['classes']
    if not os.path.exists(data_path):
        os.makedirs(data_path)
    dataset_dict = ddict(list)
    actual_max_length = 0

    if args.dataset in ['se_english']:
        hashtag_lexicon = nrc_hashtag_lexicon(args.resource_dir)
    elif args.dataset in ['se_spanish']:
        hashtag_lexicon = spanish_hashtag_lexicon(args.resource_dir)
    else:
        raise ValueError(f'Not identified dataset: {args.dataset}')
    # 0 represents no hashtag
    hashtag_dict = {h: i + 1 for i, h in enumerate(hashtag_lexicon)}
    if args.dataset == 'se_english':
        if not os.path.exists(os.path.join(args.data_dir, args.dataset, f"se_{args.split}.pkl")):
            dataset = datasets.load_dataset('sem_eval_2018_task_1', "subtask5.english")[args.split.replace('dev', 'validation')]
            with open(os.path.join(args.data_dir, args.dataset, f"se_{args.split}.pkl"), 'wb') as f:
                pickle.dump(dataset, f)
        else:
            with open(os.path.join(args.data_dir, args.dataset, f"se_{args.split}.pkl"), 'rb') as f:
                dataset = pickle.load(f)
        # datadict
        for idx in tqdm(range(0, num_examples), desc=f'Building {args.split} dataset'):
            text = f'{dataset[idx]["Tweet"]}'
            text = preprocess_dataset(text)
            encode_dict = tokenizer(text, add_special_tokens=False, return_offsets_mapping=True)
            raw_tokens = encode_dict.encodings[0].tokens
            hashtag_inputs = get_hashtag_inputs(raw_tokens, hashtag_dict)
            num_tokens = encode_dict['attention_mask'].count(1)
            input_ids = encode_dict['input_ids']
            offset_mapping = encode_dict['offset_mapping']
            if num_tokens > actual_max_length:
                actual_max_length = num_tokens

            input_length = min(len(input_ids), max_length - num_special_tokens)
            input_ids = input_ids[:input_length]
            offset_mapping = offset_mapping[:input_length]
            hashtag_inputs = hashtag_inputs[:input_length]


                # truncated_text += tokenizer.convert_ids_to_tokens(token_id).strip("▁").strip("##")
            label = [int(dataset[idx][x]) for x in dataset_info[args.dataset]['classes']]
            update_dataset_dict(
                idx, dataset_dict, input_ids, hashtag_inputs, max_length, actual_max_length, tokenizer, label,
                text, offset_mapping)

            # dataset_dict['item_idx'].append(idx)
            # dataset_dict['input_ids'].append(input_ids)
            # dataset_dict['hashtag_ids'].append(hashtag_inputs)
            # dataset_dict['attention_mask'].append(attention_mask)
            # dataset_dict['label'].append([int(dataset[idx][x]) for x in dataset_info[args.dataset]['classes']])
            # dataset_dict['offsets'].append(offset_mapping)
            # dataset_dict['truncated_texts'].append(truncated_text)

        dataset_dict['tree'] = sentiment_tree(dataset_dict['truncated_texts'],
                                           args.num_samples if args.num_samples else num_examples,
                                           dataset_dict['offsets'],
                                           max_length)

    if args.num_samples is not None and args.stratified_sampling:
        assert all([os.path.exists(os.path.join(data_path, f'{x}.pkl')) for x in ['item_idx', 'input_ids', 'hashtag_ids', 'attention_mask', 'tree', 'label']])
        dataset_dict = sample_dataset(data_path, dataset_dict, args.split, args.num_samples, args.seed)

    save_datadict(data_path, dataset_dict, args.split, args.num_samples, args.seed)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Dataset preprocessing')
    parser.add_argument('--data_dir', type=str, default='data/', help='Root directory for datasets')
    parser.add_argument('--resource_dir', type=str, default='resources/', help='Root directory for resources')
    parser.add_argument('--dataset', type=str, default='se_english',
                        choices=['se_english', 'se_arabic', 'se_spanish'])
    parser.add_argument('--arch', type=str, default='bert-base-uncased',
                        choices=['google/bigbird-roberta-base', 'bert-base-uncased'])
    parser.add_argument('--split', type=str, default='test', help='Dataset split', choices=['train', 'dev', 'test'])
    parser.add_argument('--stratified_sampling', type=bool, default=False, help='Whether to use stratified sampling')
    parser.add_argument('--num_samples', type=int, default=None,
                        help='Number of examples to sample. None means all available examples are used.')

    parser.add_argument('--seed', type=int, default=0, help='Random seed')
    parser.add_argument('--use_srl', default=True, help='Use SRL features')
    parser.add_argument('--func', default='main', help='Function to run')
    args = parser.parse_args()
    if args.func == 'main':
        main()
